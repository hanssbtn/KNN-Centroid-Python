@article{fix_1989_discriminatory,
	author = {Fix, Evelyn and Hodges, J. L.},
	month = {12},
	pages = {238},
	title = {Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties},
	doi = {10.2307/1403797},
	url = {https://www.jstor.org/stable/1403797},
	urldate = {2025-02-27},
	volume = {57},
	year = {1989},
	journal = {International Statistical Review / Revue Internationale de Statistique}
}

@misc{grant_2022_how,
	author = {Grant, Mitchell},
	editor = {James, Margaret and Li, Timothy},
	month = {10},
	title = {How Nonparametric Statistics Work},
	url = {https://www.investopedia.com/terms/n/nonparametric-statistics.asp},
	urldate = {2025-02-27},
	year = {2022},
	organization = {Investopedia}
}

@misc{googlecloud_what,
	author = {Google Cloud},
	title = {What is Supervised Learning?},
	url = {https://cloud.google.com/discover/what-is-supervised-learning},
	urldate = {2025-02-27},
	organization = {Google Cloud}
}

@incollection{BELLOTTI2014115,
	title = {Chapter 6 - Feature Selection},
	editor = {Vineeth N. Balasubramanian and Shen-Shyang Ho and Vladimir Vovk},
	booktitle = {Conformal Prediction for Reliable Machine Learning},
	publisher = {Morgan Kaufmann},
	address = {Boston},
	pages = {115-130},
	year = {2014},
	isbn = {978-0-12-398537-8},
	doi = {https://doi.org/10.1016/B978-0-12-398537-8.00006-7},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123985378000067},
	author = {Tony Bellotti and Ilia Nouretdinov and Meng Yang and Alexander Gammerman},
	keywords = {Feature Selection, Strangeness Minimization, Confidence Maximization, Support Vector Machine, Nearest Centroid Classifier, Microarray Classification},
	abstract = {In this chapter we consider the implementation of feature selection approaches within the Conformal Predictor framework. We begin with a review of feature selection, then consider several approaches to implementation. First, we use existing feature selection methods within conformal predictors, which raises some computational issues. Second, we use techniques specifically designed for conformal predictors: (1) the strangeness minimization feature selection (SMFS) method and (2) the average confidence maximization (ACM) method. SMFS minimizes the overall nonconformity values of a sequence of examples, whereas ACM maximizes the average confidence output by the conformal predictor using a subset of features. We also give some results and illustrations based on a medical dataset for abdominal pain diagnosis collected in a Scottish hospital.}
}

@book{gammerman_2016_conformal,
	author = {Gammerman, Alexander and Luo, Zhiyuan and Vega, Jesús and Vovk, Vladimir},
	month = {04},
	pages = {115-130},
	publisher = {Springer},
	title = {Conformal and Probabilistic Prediction with Applications},
	url = {https://www.sciencedirect.com/book/9780123985378/conformal-prediction-for-reliable-machine-learning},
	urldate = {2025-02-27},
	year = {2016}
}

@misc{brownlee_2023,
	author = {Brownlee, Jason},
	month = {10},
	title = {A Gentle Introduction to k-fold Cross-Validation},
	url = {https://machinelearningmastery.com/k-fold-cross-validation/},
	year = {2023},
	organization = {Machine Learning Mastery}
}

@misc{arya_2022_why,
	author = {Arya, Nisha},
	month = {07},
	title = {Why Use k-fold Cross Validation?},
	url = {https://www.kdnuggets.com/2022/07/kfold-cross-validation.html},
	year = {2022},
	organization = {KDnuggets}
}

@article{berrar_2019_crossvalidation,
	author = {Berrar, Daniel},
	pages = {542-545},
	title = {Cross-Validation},
	doi = {10.1016/b978-0-12-809633-8.20349-x},
	volume = {1},
	year = {2019},
	journal = {Encyclopedia of Bioinformatics and Computational Biology}
}

@misc{kohavi_1995_a,
	author = {Kohavi, Ron},
	publisher = {International Joint Conferences on Artificial Intelligence},
	title = {A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection},
	url = {https://www.ijcai.org/Proceedings/95-2/Papers/016.pdf},
	urldate = {2025-02-28},
	year = {1995},
	organization = {ijcai.org}
}

@incollection{HAN201239,
	title = {2 - Getting to Know Your Data},
	editor = {Jiawei Han and Micheline Kamber and Jian Pei},
	booktitle = {Data Mining (Third Edition)},
	publisher = {Morgan Kaufmann},
	edition = {Third Edition},
	address = {Boston},
	pages = {39-82},
	year = {2012},
	series = {The Morgan Kaufmann Series in Data Management Systems},
	isbn = {978-0-12-381479-1},
	doi = {https://doi.org/10.1016/B978-0-12-381479-1.00002-2},
	url = {https://www.sciencedirect.com/science/article/pii/B9780123814791000022},
	author = {Jiawei Han and Micheline Kamber and Jian Pei},
	abstract = {Publisher Summary
	This chapter is about getting familiar with the data. Knowledge about the data is useful for data preprocessing, the first major task of the data mining process. The various attribute types are studied. These include nominal attributes, binary attributes, ordinal attributes, and numeric attributes. Basic statistical descriptions can be used to learn more about each attribute's values. Given a temperature attribute, one can determine its mean (average value), median (middle value), and mode (most common value). These are measures of central tendency, which give us an idea of the “middle” or center of distribution. Knowing such basic statistics regarding each attribute makes it easier to fill in missing values, smooth noisy values, and spot outliers during data preprocessing. Knowledge of the attributes and attribute values can also help in fixing inconsistencies incurred during data integration. Plotting the measures of central tendency shows us if the data are symmetric or skewed. Quantile plots, histograms, and scatter plots are other graphic displays of basic statistical descriptions. These can all be useful during data preprocessing and can provide insight into areas for mining. The field of data visualization provides many additional techniques for viewing data through graphical means. These can help identify relations, trends, and biases “hidden” in unstructured data sets. The similarity/dissimilarity between objects may also be used to detect outliers in the data, or to perform nearest-neighbor classification. There are many measures for assessing similarity and dissimilarity. In general, such measures are referred to as proximity measures.}
}

@book{tabak2014geometry,
	title={Geometry: The Language of Space and Form},
	author={Tabak, J.},
	isbn={9780816068760},
	series={Facts on File math library},
	url={https://books.google.co.id/books?id=r0HuPiexnYwC},
	year={2014},
	pages={150},
	isbn={978-0-8160-6876-0},
	publisher={Facts On File, Incorporated}
}

@misc{stephanie_2017_mahalanobis,
  author = {Stephanie},
  month = {11},
  title = {Mahalanobis Distance: Simple Definition, Examples},
  url = {https://www.statisticshowto.com/mahalanobis-distance/},
  urldate = {2025-03-01},
  year = {2017},
  organization = {Statistics How To}
}

@article{mclachlan_1999,
	author = {Mclachlan, G.},
	year = {1999},
	month = {06},
	pages = {20-26},
	title = {Mahalanobis Distance},
	volume = {4},
	journal = {Resonance},
	doi = {10.1007/BF02834632}
}

@misc{google_2024_classification,
  author = {Google},
  publisher = {Google LLC},
  title = {Classification: Accuracy, recall, precision, and related metrics},
  url = {https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall},
  urldate = {2025-03-01},
  year = {2024},
  organization = {Google for Developers}
}

@book{olson_2008_advanced,
  author = {Olson, David L and Dursun Delen},
  edition = {1st Edition},
  month = {01},
  pages = {138},
  publisher = {Springer Science \& Business Media},
  title = {Advanced Data Mining Techniques},
  url = {https://www.google.co.id/books/edition/_/2vb-LZEn8uUC?hl=id&sa=X&ved=2ahUKEwjUmc293uiLAxUE4zgGHbrTFdEQ8fIDegQIERAF},
  urldate = {2025-03-01},
  year = {2008}
}

@MISC {cosine_euclidian,
    TITLE = {Is cosine similarity identical to l2-normalized euclidean distance?},
    AUTHOR = {Lucas (https://stats.stackexchange.com/users/7733/lucas)},
    HOWPUBLISHED = {Cross Validated},
    NOTE = {URL:https://stats.stackexchange.com/q/146279 (version: 2015-04-14)},
    EPRINT = {https://stats.stackexchange.com/q/146279},
    URL = {https://stats.stackexchange.com/q/146279}
}